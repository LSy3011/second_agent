import os
import shutil
import logging
from mem0 import Memory
from langchain_core.documents import Document
from langchain_experimental.graph_transformers import LLMGraphTransformer
from langchain_community.graphs import Neo4jGraph
from langchain_community.chat_models import ChatOllama
from mem0.embeddings.ollama import OllamaEmbedding

# ================= 1. ğŸ’‰ ä¿®å¤ç‰ˆï¼šç»´åº¦å¡«å……è¡¥ä¸ =================

print(">>> 1. æ³¨å…¥ç»´åº¦å¡«å……è¡¥ä¸ (1024 -> 1536)...")

# ä¿å­˜åŸå§‹æ–¹æ³•
original_embed = OllamaEmbedding.embed

# ğŸ”§ ä¿®å¤ç‚¹ï¼šåŠ ä¸Š *args å’Œ **kwargsï¼Œä¸ç®¡ Mem0 ä¼ å¤šå°‘ä¸ªå‚æ•°ï¼Œå…¨æ”¶ä¸‹ï¼
def patched_embed(self, text, *args, **kwargs):
    # 1. è°ƒç”¨åŸå§‹æ–¹æ³• (æŠŠæ‰€æœ‰å‚æ•°éƒ½é€ä¼ è¿‡å»)
    try:
        vector = original_embed(self, text, *args, **kwargs)
    except TypeError:
        # ä¸‡ä¸€åŸå§‹æ–¹æ³•ä¹Ÿä¸æ”¯æŒå¤šå‚æ•°ï¼Œå°±åªä¼  text
        vector = original_embed(self, text)
    
    # 2. ç»´åº¦å¡«å……é€»è¾‘ (Padding)
    target_dim = 1536 # ä¼ªè£…æˆ OpenAI çš„ç»´åº¦
    current_dim = len(vector)
    
    if current_dim < target_dim:
        pad_width = target_dim - current_dim
        # è¡¥ 0.0
        padded_vector = vector + [0.0] * pad_width
        return padded_vector
    
    return vector

# åº”ç”¨è¡¥ä¸
OllamaEmbedding.embed = patched_embed
print("   âœ… è¡¥ä¸å·²åº”ç”¨ï¼šæ”¯æŒå¤šå‚æ•°è°ƒç”¨ã€‚")

# ================= 2. é…ç½®ä¸æ¸…ç† =================

NEO4J_URL = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASSWORD = "password123456"
VECTOR_DB_PATH = "./my_agent_vector_padding"

# æ¸…ç†æ—§æ•°æ®ï¼Œä¿è¯å…¨æ–°å¼€å§‹
if os.path.exists(VECTOR_DB_PATH):
    shutil.rmtree(VECTOR_DB_PATH)

# ================= 3. å¯åŠ¨ Agent =================

print(">>> 2. å¯åŠ¨æ··åˆè®°å¿† Agent...")

try:
    # A. åˆå§‹åŒ– Mem0
    # æˆ‘ä»¬æ•…æ„è®¾ç½® embedding_dims ä¸º 1536ï¼Œé…åˆæˆ‘ä»¬çš„è¡¥ä¸
    mem0 = Memory.from_config({
        "vector_store": {
            "provider": "qdrant",
            "config": {
                "path": VECTOR_DB_PATH,
                "collection_name": "mem0"
            }
        },
        "llm": {
            "provider": "ollama",
            "config": {"model": "qwen2.5:7b", "temperature": 0}
        },
        "embedder": {
            "provider": "ollama",
            "config": {
                "model": "bge-m3:latest",
                "embedding_dims": 1536 
            }
        }
    })
    print("   âœ… Mem0 (å‘é‡å±‚) å°±ç»ª")

    # B. åˆå§‹åŒ– LangChain
    llm = ChatOllama(model="qwen2.5:7b", temperature=0, format="json")
    llm_transformer = LLMGraphTransformer(llm=llm)
    graph = Neo4jGraph(url=NEO4J_URL, username=NEO4J_USER, password=NEO4J_PASSWORD)
    print("   âœ… LangChain + Neo4j (å›¾è°±å±‚) å°±ç»ª")

except Exception as e:
    print(f"âŒ åˆå§‹åŒ–æŠ¥é”™: {e}")
    exit()

# ================= 4. æ‰§è¡Œå†™å…¥æµ‹è¯• =================

def process(user_id, text):
    print(f"\n--- å¤„ç†: {text} ---")
    
    # 1. å‘é‡å†™å…¥
    try:
        mem0.add(text, user_id=user_id)
        # å¦‚æœè¿™é‡ŒæˆåŠŸï¼Œè¯´æ˜å‚æ•°ä¼ é€’é—®é¢˜å’Œç»´åº¦é—®é¢˜éƒ½è§£å†³äº†
        print("   âœ… [Mem0] å‘é‡å†™å…¥æˆåŠŸ")
    except Exception as e:
        print(f"   âŒ [Mem0] å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    # 2. å›¾è°±å†™å…¥
    try:
        docs = [Document(page_content=text)]
        g_docs = llm_transformer.convert_to_graph_documents(docs)
        if g_docs:
            graph.add_graph_documents(g_docs)
            print(f"   âœ… [Neo4j] å›¾è°±å†™å…¥æˆåŠŸ ({len(g_docs[0].relationships)} å…³ç³»)")
            for r in g_docs[0].relationships:
                print(f"      ğŸ”— {r.source.id} --[{r.type}]--> {r.target.id}")
    except Exception as e:
        print(f"   âŒ [Neo4j] å¤±è´¥: {e}")

if __name__ == "__main__":
    process("padding_user_final", "Alex works as a Python Developer.")
    process("padding_user_final", "Alex loves Neo4j.")
    print("\n>>> ğŸ‰ éªŒè¯å®Œæˆï¼è¿™æ¬¡ä¸€å®šè¡Œï¼")
